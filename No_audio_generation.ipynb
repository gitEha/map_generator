{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Input, concatenate, Dropout, SpatialDropout1D, Flatten, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_DIR_PATH = 'MAPS/PROTOTYPE MAPS/'\n",
    "MAP_NAME = 'Levels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376,307,14036\n",
      "176,192,14888\n",
      "336,192,15101\n",
      "200,248,15314\n",
      "312,136,15527\n",
      "256,272,15741\n",
      "256,112,15954\n",
      "312,248,16167\n",
      "200,136,16380\n",
      "56,56,16806\n",
      "456,328,17445\n",
      "104,192,18297\n",
      "412,192,18723\n",
      "256,28,19362\n",
      "256,161\n"
     ]
    }
   ],
   "source": [
    "# Load the beatmap hitcirlces\n",
    "\n",
    "with open(os.path.join(MAP_DIR_PATH, MAP_NAME + '.osu')) as f:\n",
    "    content = [x.strip() for x in f.readlines()]\n",
    "\n",
    "hitobjects_index = [x for x in content].index('[HitObjects]')\n",
    "X_one_example = '\\n'.join([','.join(x.split(',')[:3]) for x in content[hitobjects_index + 1:]])\n",
    "\n",
    "print(X_one_example[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 3176\n",
      "376,307,14036\n",
      "\n",
      "------------\n",
      "76,307,14036\n",
      "1\n",
      "------------\n",
      "6,307,14036\n",
      "17\n",
      "------------\n",
      ",307,14036\n",
      "176\n",
      "------------\n",
      "307,14036\n",
      "176,\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "length = 13\n",
    "sequences = list()\n",
    "for i in range(length, len(X_one_example)):\n",
    "    # select sequence of tokens\n",
    "    seq = X_one_example[i-length:i+1]\n",
    "    # store\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "for i in range(5):\n",
    "    print(sequences[i])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 9, 8, 1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0]\n",
      "------------\n",
      "[9, 8, 1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3]\n",
      "------------\n",
      "[8, 1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3, 9]\n",
      "------------\n",
      "[1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3, 9, 8]\n",
      "------------\n",
      "[5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3, 9, 8, 1]\n",
      "------------\n",
      "Vocabulary Size: 12\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(X_one_example)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "sequences_mapped = list()\n",
    "for seq in sequences:\n",
    "    # integer encode line\n",
    "    encoded_seq = [mapping[char] for char in seq]\n",
    "    # store\n",
    "    sequences_mapped.append(encoded_seq)\n",
    "\n",
    "for i in range(5):\n",
    "    print(sequences_mapped[i])\n",
    "    print('------------')\n",
    "    \n",
    "\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 1 5 2 9 1 3 6 2 5 8 0 3]\n",
      "9\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences_mapped)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "print(X[2])\n",
    "print(y[2])\n",
    "\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "print(X[2])\n",
    "print(y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 75)                26400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                912       \n",
      "=================================================================\n",
      "Total params: 27,312\n",
      "Trainable params: 27,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3176/3176 [==============================] - 6s 2ms/step - loss: 2.4246 - acc: 0.1612\n",
      "Epoch 2/150\n",
      "3176/3176 [==============================] - 2s 651us/step - loss: 2.2476 - acc: 0.2538\n",
      "Epoch 3/150\n",
      "3176/3176 [==============================] - 2s 651us/step - loss: 2.0133 - acc: 0.3060\n",
      "Epoch 4/150\n",
      "3176/3176 [==============================] - 2s 658us/step - loss: 1.9248 - acc: 0.3256\n",
      "Epoch 5/150\n",
      "3176/3176 [==============================] - 2s 657us/step - loss: 1.8458 - acc: 0.3382\n",
      "Epoch 6/150\n",
      "3176/3176 [==============================] - 2s 650us/step - loss: 1.8066 - acc: 0.3457\n",
      "Epoch 7/150\n",
      "3176/3176 [==============================] - 2s 664us/step - loss: 1.7742 - acc: 0.3583\n",
      "Epoch 8/150\n",
      "3176/3176 [==============================] - 2s 663us/step - loss: 1.7545 - acc: 0.3684\n",
      "Epoch 9/150\n",
      "3176/3176 [==============================] - 2s 668us/step - loss: 1.7280 - acc: 0.3706\n",
      "Epoch 10/150\n",
      "3176/3176 [==============================] - 2s 697us/step - loss: 1.7154 - acc: 0.3788\n",
      "Epoch 11/150\n",
      "3176/3176 [==============================] - 2s 675us/step - loss: 1.6962 - acc: 0.3819\n",
      "Epoch 12/150\n",
      "3176/3176 [==============================] - 2s 667us/step - loss: 1.6813 - acc: 0.3889\n",
      "Epoch 13/150\n",
      "3176/3176 [==============================] - 2s 662us/step - loss: 1.6648 - acc: 0.3926\n",
      "Epoch 14/150\n",
      "3176/3176 [==============================] - 2s 659us/step - loss: 1.6515 - acc: 0.4014\n",
      "Epoch 15/150\n",
      "3176/3176 [==============================] - 2s 661us/step - loss: 1.6374 - acc: 0.4030\n",
      "Epoch 16/150\n",
      "3176/3176 [==============================] - 2s 659us/step - loss: 1.6264 - acc: 0.4090\n",
      "Epoch 17/150\n",
      "3176/3176 [==============================] - 2s 652us/step - loss: 1.6184 - acc: 0.4137\n",
      "Epoch 18/150\n",
      "3176/3176 [==============================] - 2s 673us/step - loss: 1.6138 - acc: 0.4131\n",
      "Epoch 19/150\n",
      "3176/3176 [==============================] - 2s 658us/step - loss: 1.6038 - acc: 0.4128\n",
      "Epoch 20/150\n",
      "3176/3176 [==============================] - 2s 661us/step - loss: 1.5922 - acc: 0.4232\n",
      "Epoch 21/150\n",
      "3176/3176 [==============================] - 2s 661us/step - loss: 1.5810 - acc: 0.4266\n",
      "Epoch 22/150\n",
      "3176/3176 [==============================] - 2s 663us/step - loss: 1.5733 - acc: 0.4235\n",
      "Epoch 23/150\n",
      "3176/3176 [==============================] - 2s 669us/step - loss: 1.5577 - acc: 0.4358\n",
      "Epoch 24/150\n",
      "3176/3176 [==============================] - 2s 661us/step - loss: 1.5545 - acc: 0.4408\n",
      "Epoch 25/150\n",
      "3176/3176 [==============================] - 2s 650us/step - loss: 1.5428 - acc: 0.4430\n",
      "Epoch 26/150\n",
      "3176/3176 [==============================] - 2s 667us/step - loss: 1.5360 - acc: 0.4477\n",
      "Epoch 27/150\n",
      "3176/3176 [==============================] - 2s 659us/step - loss: 1.5323 - acc: 0.4452\n",
      "Epoch 28/150\n",
      "3176/3176 [==============================] - 2s 667us/step - loss: 1.5170 - acc: 0.4528\n",
      "Epoch 29/150\n",
      "3176/3176 [==============================] - 2s 651us/step - loss: 1.5034 - acc: 0.4597\n",
      "Epoch 30/150\n",
      "3176/3176 [==============================] - 2s 678us/step - loss: 1.4980 - acc: 0.4622\n",
      "Epoch 31/150\n",
      "3176/3176 [==============================] - 2s 660us/step - loss: 1.4880 - acc: 0.4588\n",
      "Epoch 32/150\n",
      "3176/3176 [==============================] - 2s 654us/step - loss: 1.4724 - acc: 0.4676\n",
      "Epoch 33/150\n",
      "3176/3176 [==============================] - 2s 667us/step - loss: 1.4620 - acc: 0.4764\n",
      "Epoch 34/150\n",
      "3176/3176 [==============================] - 2s 667us/step - loss: 1.4582 - acc: 0.4710\n",
      "Epoch 35/150\n",
      "3176/3176 [==============================] - 2s 658us/step - loss: 1.5227 - acc: 0.4572\n",
      "Epoch 36/150\n",
      "3176/3176 [==============================] - 2s 662us/step - loss: 1.4313 - acc: 0.4865\n",
      "Epoch 37/150\n",
      "3176/3176 [==============================] - 2s 655us/step - loss: 1.4179 - acc: 0.4934\n",
      "Epoch 38/150\n",
      "3176/3176 [==============================] - 2s 659us/step - loss: 1.4064 - acc: 0.4934\n",
      "Epoch 39/150\n",
      "3176/3176 [==============================] - 2s 649us/step - loss: 1.3974 - acc: 0.5016\n",
      "Epoch 40/150\n",
      "3176/3176 [==============================] - 2s 679us/step - loss: 1.3746 - acc: 0.5079\n",
      "Epoch 41/150\n",
      "3176/3176 [==============================] - 2s 674us/step - loss: 1.3643 - acc: 0.5091\n",
      "Epoch 42/150\n",
      "3176/3176 [==============================] - 2s 657us/step - loss: 1.3621 - acc: 0.5116\n",
      "Epoch 43/150\n",
      "3176/3176 [==============================] - 2s 656us/step - loss: 1.3402 - acc: 0.5280\n",
      "Epoch 44/150\n",
      "3176/3176 [==============================] - 2s 654us/step - loss: 1.3231 - acc: 0.5205\n",
      "Epoch 45/150\n",
      "3176/3176 [==============================] - 2s 671us/step - loss: 1.3075 - acc: 0.5331\n",
      "Epoch 46/150\n",
      "3176/3176 [==============================] - 2s 649us/step - loss: 1.3022 - acc: 0.5416\n",
      "Epoch 47/150\n",
      "3176/3176 [==============================] - 2s 659us/step - loss: 1.2715 - acc: 0.5548\n",
      "Epoch 48/150\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 1.2653 - acc: 0.5567\n",
      "Epoch 49/150\n",
      "3176/3176 [==============================] - 2s 658us/step - loss: 1.2405 - acc: 0.5633\n",
      "Epoch 50/150\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 1.2249 - acc: 0.5699\n",
      "Epoch 51/150\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 1.2083 - acc: 0.5850\n",
      "Epoch 52/150\n",
      "3176/3176 [==============================] - 2s 655us/step - loss: 1.1903 - acc: 0.5825\n",
      "Epoch 53/150\n",
      "3176/3176 [==============================] - 2s 721us/step - loss: 1.1728 - acc: 0.5960\n",
      "Epoch 54/150\n",
      "3176/3176 [==============================] - 2s 653us/step - loss: 1.1492 - acc: 0.6137\n",
      "Epoch 55/150\n",
      "3176/3176 [==============================] - 2s 630us/step - loss: 1.1362 - acc: 0.6093\n",
      "Epoch 56/150\n",
      "3176/3176 [==============================] - 2s 646us/step - loss: 1.1180 - acc: 0.6206\n",
      "Epoch 57/150\n",
      "3176/3176 [==============================] - 2s 644us/step - loss: 1.0984 - acc: 0.6247\n",
      "Epoch 58/150\n",
      "3176/3176 [==============================] - 2s 670us/step - loss: 1.0808 - acc: 0.6291\n",
      "Epoch 59/150\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 1.0610 - acc: 0.6429\n",
      "Epoch 60/150\n",
      "3176/3176 [==============================] - 2s 638us/step - loss: 1.0495 - acc: 0.6452\n",
      "Epoch 61/150\n",
      "3176/3176 [==============================] - 2s 640us/step - loss: 1.0167 - acc: 0.6559\n",
      "Epoch 62/150\n",
      "3176/3176 [==============================] - 2s 661us/step - loss: 1.0124 - acc: 0.6615\n",
      "Epoch 63/150\n",
      "3176/3176 [==============================] - 2s 648us/step - loss: 0.9768 - acc: 0.6770\n",
      "Epoch 64/150\n",
      "3176/3176 [==============================] - 2s 634us/step - loss: 0.9616 - acc: 0.6814\n",
      "Epoch 65/150\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 0.9404 - acc: 0.6842\n",
      "Epoch 66/150\n",
      "3176/3176 [==============================] - 2s 642us/step - loss: 0.9137 - acc: 0.7018\n",
      "Epoch 67/150\n",
      "3176/3176 [==============================] - 2s 660us/step - loss: 0.8934 - acc: 0.7106\n",
      "Epoch 68/150\n",
      "3176/3176 [==============================] - 2s 632us/step - loss: 0.8666 - acc: 0.7229\n",
      "Epoch 69/150\n",
      "3176/3176 [==============================] - 2s 620us/step - loss: 0.8457 - acc: 0.7264\n",
      "Epoch 70/150\n",
      "3176/3176 [==============================] - 2s 656us/step - loss: 0.8122 - acc: 0.7412\n",
      "Epoch 71/150\n",
      "3176/3176 [==============================] - 2s 656us/step - loss: 0.7972 - acc: 0.7547\n",
      "Epoch 72/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.7891 - acc: 0.7528\n",
      "Epoch 73/150\n",
      "3176/3176 [==============================] - 2s 647us/step - loss: 0.7647 - acc: 0.7610\n",
      "Epoch 74/150\n",
      "3176/3176 [==============================] - 2s 618us/step - loss: 0.7321 - acc: 0.7812\n",
      "Epoch 75/150\n",
      "3176/3176 [==============================] - 2s 659us/step - loss: 0.7143 - acc: 0.7824\n",
      "Epoch 76/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.6978 - acc: 0.7909\n",
      "Epoch 77/150\n",
      "3176/3176 [==============================] - 2s 626us/step - loss: 0.6637 - acc: 0.8045\n",
      "Epoch 78/150\n",
      "3176/3176 [==============================] - 2s 662us/step - loss: 0.6489 - acc: 0.8105\n",
      "Epoch 79/150\n",
      "3176/3176 [==============================] - 2s 629us/step - loss: 0.6378 - acc: 0.8136\n",
      "Epoch 80/150\n",
      "3176/3176 [==============================] - 2s 690us/step - loss: 0.6302 - acc: 0.8174\n",
      "Epoch 81/150\n",
      "3176/3176 [==============================] - 2s 688us/step - loss: 0.6003 - acc: 0.8290\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3176/3176 [==============================] - 2s 626us/step - loss: 0.5775 - acc: 0.8391\n",
      "Epoch 83/150\n",
      "3176/3176 [==============================] - 2s 617us/step - loss: 0.5488 - acc: 0.8467\n",
      "Epoch 84/150\n",
      "3176/3176 [==============================] - 2s 630us/step - loss: 0.5258 - acc: 0.8555\n",
      "Epoch 85/150\n",
      "3176/3176 [==============================] - 2s 635us/step - loss: 0.5131 - acc: 0.8630\n",
      "Epoch 86/150\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 0.4948 - acc: 0.8681\n",
      "Epoch 87/150\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 0.4732 - acc: 0.8766\n",
      "Epoch 88/150\n",
      "3176/3176 [==============================] - 2s 698us/step - loss: 0.4486 - acc: 0.8945\n",
      "Epoch 89/150\n",
      "3176/3176 [==============================] - 2s 641us/step - loss: 0.4357 - acc: 0.8955\n",
      "Epoch 90/150\n",
      "3176/3176 [==============================] - 2s 631us/step - loss: 0.4166 - acc: 0.8992\n",
      "Epoch 91/150\n",
      "3176/3176 [==============================] - 2s 630us/step - loss: 0.3950 - acc: 0.9077\n",
      "Epoch 92/150\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 0.3809 - acc: 0.9169\n",
      "Epoch 93/150\n",
      "3176/3176 [==============================] - 2s 631us/step - loss: 0.3836 - acc: 0.9068\n",
      "Epoch 94/150\n",
      "3176/3176 [==============================] - 2s 657us/step - loss: 0.3572 - acc: 0.9178\n",
      "Epoch 95/150\n",
      "3176/3176 [==============================] - 2s 641us/step - loss: 0.3368 - acc: 0.9295\n",
      "Epoch 96/150\n",
      "3176/3176 [==============================] - 2s 677us/step - loss: 0.3318 - acc: 0.9348\n",
      "Epoch 97/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.3050 - acc: 0.9424\n",
      "Epoch 98/150\n",
      "3176/3176 [==============================] - 2s 634us/step - loss: 0.2912 - acc: 0.9430\n",
      "Epoch 99/150\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 0.2773 - acc: 0.9493\n",
      "Epoch 100/150\n",
      "3176/3176 [==============================] - 2s 657us/step - loss: 0.2761 - acc: 0.9449\n",
      "Epoch 101/150\n",
      "3176/3176 [==============================] - 2s 658us/step - loss: 0.2549 - acc: 0.9556\n",
      "Epoch 102/150\n",
      "3176/3176 [==============================] - 2s 633us/step - loss: 0.2299 - acc: 0.9657\n",
      "Epoch 103/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.2175 - acc: 0.9682\n",
      "Epoch 104/150\n",
      "3176/3176 [==============================] - 2s 634us/step - loss: 0.2045 - acc: 0.9720\n",
      "Epoch 105/150\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 0.1951 - acc: 0.9736\n",
      "Epoch 106/150\n",
      "3176/3176 [==============================] - 2s 638us/step - loss: 0.1876 - acc: 0.9736\n",
      "Epoch 107/150\n",
      "3176/3176 [==============================] - 2s 645us/step - loss: 0.2312 - acc: 0.9575\n",
      "Epoch 108/150\n",
      "3176/3176 [==============================] - 2s 630us/step - loss: 0.2250 - acc: 0.9591\n",
      "Epoch 109/150\n",
      "3176/3176 [==============================] - 2s 634us/step - loss: 0.1889 - acc: 0.9726\n",
      "Epoch 110/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.1562 - acc: 0.9817\n",
      "Epoch 111/150\n",
      "3176/3176 [==============================] - 2s 632us/step - loss: 0.1407 - acc: 0.9865\n",
      "Epoch 112/150\n",
      "3176/3176 [==============================] - 2s 629us/step - loss: 0.1287 - acc: 0.9855\n",
      "Epoch 113/150\n",
      "3176/3176 [==============================] - 2s 658us/step - loss: 0.1226 - acc: 0.9896\n",
      "Epoch 114/150\n",
      "3176/3176 [==============================] - 2s 631us/step - loss: 0.1152 - acc: 0.9909\n",
      "Epoch 115/150\n",
      "3176/3176 [==============================] - 2s 627us/step - loss: 0.1022 - acc: 0.9918\n",
      "Epoch 116/150\n",
      "3176/3176 [==============================] - 2s 642us/step - loss: 0.0964 - acc: 0.9934\n",
      "Epoch 117/150\n",
      "3176/3176 [==============================] - 2s 635us/step - loss: 0.0943 - acc: 0.9937\n",
      "Epoch 118/150\n",
      "3176/3176 [==============================] - 2s 638us/step - loss: 0.0879 - acc: 0.9943\n",
      "Epoch 119/150\n",
      "3176/3176 [==============================] - 2s 623us/step - loss: 0.0852 - acc: 0.9943\n",
      "Epoch 120/150\n",
      "3176/3176 [==============================] - 2s 638us/step - loss: 0.1000 - acc: 0.9921\n",
      "Epoch 121/150\n",
      "3176/3176 [==============================] - 2s 649us/step - loss: 0.1194 - acc: 0.9855\n",
      "Epoch 122/150\n",
      "3176/3176 [==============================] - 2s 649us/step - loss: 0.2778 - acc: 0.9279\n",
      "Epoch 123/150\n",
      "3176/3176 [==============================] - 2s 644us/step - loss: 0.2073 - acc: 0.9496\n",
      "Epoch 124/150\n",
      "3176/3176 [==============================] - 2s 624us/step - loss: 0.1653 - acc: 0.9682\n",
      "Epoch 125/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.0739 - acc: 0.9959\n",
      "Epoch 126/150\n",
      "3176/3176 [==============================] - 2s 633us/step - loss: 0.0570 - acc: 0.9997\n",
      "Epoch 127/150\n",
      "3176/3176 [==============================] - 2s 657us/step - loss: 0.0507 - acc: 0.9991\n",
      "Epoch 128/150\n",
      "3176/3176 [==============================] - 2s 644us/step - loss: 0.0468 - acc: 0.9991\n",
      "Epoch 129/150\n",
      "3176/3176 [==============================] - 2s 678us/step - loss: 0.0424 - acc: 0.9991\n",
      "Epoch 130/150\n",
      "3176/3176 [==============================] - 2s 653us/step - loss: 0.0409 - acc: 1.0000\n",
      "Epoch 131/150\n",
      "3176/3176 [==============================] - 2s 638us/step - loss: 0.0378 - acc: 0.9997\n",
      "Epoch 132/150\n",
      "3176/3176 [==============================] - 2s 648us/step - loss: 0.0362 - acc: 1.0000\n",
      "Epoch 133/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.0350 - acc: 0.9987\n",
      "Epoch 134/150\n",
      "3176/3176 [==============================] - 2s 647us/step - loss: 0.0354 - acc: 0.9991\n",
      "Epoch 135/150\n",
      "3176/3176 [==============================] - 2s 652us/step - loss: 0.0319 - acc: 0.9997\n",
      "Epoch 136/150\n",
      "3176/3176 [==============================] - 2s 639us/step - loss: 0.0296 - acc: 0.9997\n",
      "Epoch 137/150\n",
      "3176/3176 [==============================] - 2s 647us/step - loss: 0.0288 - acc: 1.0000\n",
      "Epoch 138/150\n",
      "3176/3176 [==============================] - 2s 622us/step - loss: 0.0291 - acc: 0.9997\n",
      "Epoch 139/150\n",
      "3176/3176 [==============================] - 2s 644us/step - loss: 0.0266 - acc: 1.0000\n",
      "Epoch 140/150\n",
      "3176/3176 [==============================] - 2s 629us/step - loss: 0.0288 - acc: 0.9997\n",
      "Epoch 141/150\n",
      "3176/3176 [==============================] - 2s 644us/step - loss: 0.0240 - acc: 1.0000\n",
      "Epoch 142/150\n",
      "3176/3176 [==============================] - 2s 642us/step - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 143/150\n",
      "3176/3176 [==============================] - 2s 639us/step - loss: 0.0217 - acc: 0.9994\n",
      "Epoch 144/150\n",
      "3176/3176 [==============================] - 2s 631us/step - loss: 0.0210 - acc: 1.0000\n",
      "Epoch 145/150\n",
      "3176/3176 [==============================] - 2s 656us/step - loss: 0.3820 - acc: 0.8863\n",
      "Epoch 146/150\n",
      "3176/3176 [==============================] - 2s 679us/step - loss: 0.4891 - acc: 0.8404\n",
      "Epoch 147/150\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 0.1367 - acc: 0.9717\n",
      "Epoch 148/150\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 0.0820 - acc: 0.9902\n",
      "Epoch 149/150\n",
      "3176/3176 [==============================] - 2s 678us/step - loss: 0.0370 - acc: 0.9997\n",
      "Epoch 150/150\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 0.0267 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d1b8be2550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355,355,12356,200\n",
      "\n",
      "174,192,41600\n",
      "256,300,64443\n",
      "256,280,64756\n",
      "220,286,66709\n",
      "488,64,670,551924164\n",
      "2333,38,84770\n",
      "\n",
      "256,28,47405\n",
      "256,320,74433\n",
      "158,288,47598\n",
      "256,280,72551\n",
      "202,140,52950\n",
      "256,44,19004\n",
      "256,192,19888\n",
      "336,171,15101\n",
      "340,234,15667\n",
      "210,232,88077\n",
      "473,391,87830\n",
      "409,118,84007\n",
      "339,318,80085\n",
      "439,919,94775\n",
      "444,144,831,1\n",
      "220,2362,2593\n",
      "110,224,37551\n",
      "332,240,22555\n",
      "160,240,52299\n",
      "312,122,43268\n",
      "156,128,40906\n",
      "332,286,70859\n",
      "128,292,22788\n",
      "256,192,27840\n",
      "356,492,74541\n",
      "272,88,26666\n",
      "256,192,88473\n",
      "336,192,47811\n",
      "320,336,80864\n",
      "279,219,87670\n",
      "330,192,66403\n",
      "256,300,64453\n",
      "256,280,64556\n",
      "220,180,92775\n",
      "58,444,3732\n",
      "256,1404,3251,244,12\n",
      "2\n",
      "\n",
      "40,286,99911\n",
      "422,43,39518\n",
      "224,,76,703\n",
      "256,256,5640\n",
      "\n",
      "429,192,30155\n",
      "392,104,22741\n",
      "364,288,26588\n",
      "336,291,84107\n",
      "433,111,40157\n",
      "144,\n",
      "36,02301\n",
      "448,248,3651,\n",
      "252\n",
      "2604,112,19984\n",
      "6,14172622,3,32,400\n",
      "15248,408,2271\n",
      "\n",
      "280,288,7989\n",
      "\n",
      "48,140,87071\n",
      "39,2111,85298\n",
      "332,305,40447\n",
      "176,292,40993\n",
      "132,88,40335\n",
      "256,280,42459\n",
      "328,288,8299\n",
      "\n",
      "256,57,30464\n",
      "256,192,34751\n",
      "172,336,85887\n",
      "336,215,79166\n",
      "256,200,48429,2562\n",
      "60,2\n"
     ]
    }
   ],
   "source": [
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "        # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text\n",
    "\n",
    "# test start of rhyme\n",
    "print(generate_seq(model, mapping, length, '355,355,12356', 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
