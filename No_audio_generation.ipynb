{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Input, concatenate, Dropout, SpatialDropout1D, Flatten, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_DIR_PATH = 'MAPS/PROTOTYPE MAPS/'\n",
    "MAP_NAME = 'Levels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376,307,14036\n",
      "176,192,14888\n",
      "336,192,15101\n",
      "200,248,15314\n",
      "312,136,15527\n",
      "256,272,15741\n",
      "256,112,15954\n",
      "312,248,16167\n",
      "200,136,16380\n",
      "56,56,16806\n",
      "456,328,17445\n",
      "104,192,18297\n",
      "412,192,18723\n",
      "256,28,19362\n",
      "256,161\n"
     ]
    }
   ],
   "source": [
    "# Load the beatmap hitcirlces\n",
    "\n",
    "with open(os.path.join(MAP_DIR_PATH, MAP_NAME + '.osu')) as f:\n",
    "    content = [x.strip() for x in f.readlines()]\n",
    "\n",
    "hitobjects_index = [x for x in content].index('[HitObjects]')\n",
    "X_one_example = '\\n'.join([','.join(x.split(',')[:3]) for x in content[hitobjects_index + 1:]])\n",
    "\n",
    "print(X_one_example[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 3176\n",
      "376,307,14036\n",
      "\n",
      "------------\n",
      "76,307,14036\n",
      "1\n",
      "------------\n",
      "6,307,14036\n",
      "17\n",
      "------------\n",
      ",307,14036\n",
      "176\n",
      "------------\n",
      "307,14036\n",
      "176,\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "length = 13\n",
    "sequences = list()\n",
    "for i in range(length, len(X_one_example)):\n",
    "    # select sequence of tokens\n",
    "    seq = X_one_example[i-length:i+1]\n",
    "    # store\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "for i in range(5):\n",
    "    print(sequences[i])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 9, 8, 1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0]\n",
      "------------\n",
      "[9, 8, 1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3]\n",
      "------------\n",
      "[8, 1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3, 9]\n",
      "------------\n",
      "[1, 5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3, 9, 8]\n",
      "------------\n",
      "[5, 2, 9, 1, 3, 6, 2, 5, 8, 0, 3, 9, 8, 1]\n",
      "------------\n",
      "Vocabulary Size: 12\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(X_one_example)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "sequences_mapped = list()\n",
    "for seq in sequences:\n",
    "    # integer encode line\n",
    "    encoded_seq = [mapping[char] for char in seq]\n",
    "    # store\n",
    "    sequences_mapped.append(encoded_seq)\n",
    "\n",
    "for i in range(5):\n",
    "    print(sequences_mapped[i])\n",
    "    print('------------')\n",
    "    \n",
    "\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 1 5 2 9 1 3 6 2 5 8 0 3]\n",
      "9\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences_mapped)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "print(X[2])\n",
    "print(y[2])\n",
    "\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "print(X[2])\n",
    "print(y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 75)                26400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                912       \n",
      "=================================================================\n",
      "Total params: 27,312\n",
      "Trainable params: 27,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3176/3176 [==============================] - 4s 1ms/step - loss: 2.4341 - acc: 0.1436\n",
      "Epoch 2/100\n",
      "3176/3176 [==============================] - 2s 617us/step - loss: 2.2784 - acc: 0.2305\n",
      "Epoch 3/100\n",
      "3176/3176 [==============================] - 2s 640us/step - loss: 2.0897 - acc: 0.2824\n",
      "Epoch 4/100\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 1.9130 - acc: 0.3098\n",
      "Epoch 5/100\n",
      "3176/3176 [==============================] - 2s 620us/step - loss: 1.8283 - acc: 0.3375\n",
      "Epoch 6/100\n",
      "3176/3176 [==============================] - 2s 632us/step - loss: 1.7771 - acc: 0.3495\n",
      "Epoch 7/100\n",
      "3176/3176 [==============================] - 2s 632us/step - loss: 1.7642 - acc: 0.3558\n",
      "Epoch 8/100\n",
      "3176/3176 [==============================] - 2s 633us/step - loss: 1.7384 - acc: 0.3630\n",
      "Epoch 9/100\n",
      "3176/3176 [==============================] - 2s 632us/step - loss: 1.7199 - acc: 0.3700\n",
      "Epoch 10/100\n",
      "3176/3176 [==============================] - 2s 627us/step - loss: 1.7002 - acc: 0.3800\n",
      "Epoch 11/100\n",
      "3176/3176 [==============================] - 2s 620us/step - loss: 1.7127 - acc: 0.3819\n",
      "Epoch 12/100\n",
      "3176/3176 [==============================] - 2s 621us/step - loss: 1.6794 - acc: 0.3895\n",
      "Epoch 13/100\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 1.6599 - acc: 0.3996\n",
      "Epoch 14/100\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 1.6560 - acc: 0.3999\n",
      "Epoch 15/100\n",
      "3176/3176 [==============================] - 2s 641us/step - loss: 1.6437 - acc: 0.4014\n",
      "Epoch 16/100\n",
      "3176/3176 [==============================] - 2s 643us/step - loss: 1.6313 - acc: 0.4043\n",
      "Epoch 17/100\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 1.6212 - acc: 0.4059\n",
      "Epoch 18/100\n",
      "3176/3176 [==============================] - 2s 639us/step - loss: 1.6150 - acc: 0.4159\n",
      "Epoch 19/100\n",
      "3176/3176 [==============================] - 2s 647us/step - loss: 1.6056 - acc: 0.4166\n",
      "Epoch 20/100\n",
      "3176/3176 [==============================] - 2s 655us/step - loss: 1.5981 - acc: 0.4159\n",
      "Epoch 21/100\n",
      "3176/3176 [==============================] - 2s 633us/step - loss: 1.5863 - acc: 0.4203\n",
      "Epoch 22/100\n",
      "3176/3176 [==============================] - 2s 645us/step - loss: 1.5787 - acc: 0.4232\n",
      "Epoch 23/100\n",
      "3176/3176 [==============================] - 2s 627us/step - loss: 1.5738 - acc: 0.4270\n",
      "Epoch 24/100\n",
      "3176/3176 [==============================] - 2s 624us/step - loss: 1.5666 - acc: 0.4383\n",
      "Epoch 25/100\n",
      "3176/3176 [==============================] - 2s 657us/step - loss: 1.5632 - acc: 0.4405\n",
      "Epoch 26/100\n",
      "3176/3176 [==============================] - 2s 639us/step - loss: 1.5472 - acc: 0.4386\n",
      "Epoch 27/100\n",
      "3176/3176 [==============================] - 2s 650us/step - loss: 1.5362 - acc: 0.4399\n",
      "Epoch 28/100\n",
      "3176/3176 [==============================] - 2s 663us/step - loss: 1.5461 - acc: 0.4430\n",
      "Epoch 29/100\n",
      "3176/3176 [==============================] - 2s 641us/step - loss: 1.5171 - acc: 0.4446\n",
      "Epoch 30/100\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 1.5108 - acc: 0.4550\n",
      "Epoch 31/100\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 1.5030 - acc: 0.4581\n",
      "Epoch 32/100\n",
      "3176/3176 [==============================] - 2s 664us/step - loss: 1.4925 - acc: 0.4559\n",
      "Epoch 33/100\n",
      "3176/3176 [==============================] - 2s 629us/step - loss: 1.4847 - acc: 0.4606\n",
      "Epoch 34/100\n",
      "3176/3176 [==============================] - 2s 626us/step - loss: 1.4821 - acc: 0.4622\n",
      "Epoch 35/100\n",
      "3176/3176 [==============================] - 2s 670us/step - loss: 1.4729 - acc: 0.4616\n",
      "Epoch 36/100\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 1.4487 - acc: 0.4698\n",
      "Epoch 37/100\n",
      "3176/3176 [==============================] - 2s 623us/step - loss: 1.4410 - acc: 0.4780\n",
      "Epoch 38/100\n",
      "3176/3176 [==============================] - 2s 650us/step - loss: 1.4317 - acc: 0.4830\n",
      "Epoch 39/100\n",
      "3176/3176 [==============================] - 2s 624us/step - loss: 1.4295 - acc: 0.4896\n",
      "Epoch 40/100\n",
      "3176/3176 [==============================] - 2s 639us/step - loss: 1.4068 - acc: 0.4921\n",
      "Epoch 41/100\n",
      "3176/3176 [==============================] - 2s 664us/step - loss: 1.4126 - acc: 0.4896\n",
      "Epoch 42/100\n",
      "3176/3176 [==============================] - 2s 629us/step - loss: 1.3819 - acc: 0.5047\n",
      "Epoch 43/100\n",
      "3176/3176 [==============================] - 2s 624us/step - loss: 1.3680 - acc: 0.5154\n",
      "Epoch 44/100\n",
      "3176/3176 [==============================] - 2s 622us/step - loss: 1.3663 - acc: 0.5123\n",
      "Epoch 45/100\n",
      "3176/3176 [==============================] - 2s 624us/step - loss: 1.3529 - acc: 0.5202\n",
      "Epoch 46/100\n",
      "3176/3176 [==============================] - 2s 631us/step - loss: 1.3341 - acc: 0.5252\n",
      "Epoch 47/100\n",
      "3176/3176 [==============================] - 2s 628us/step - loss: 1.3148 - acc: 0.5293\n",
      "Epoch 48/100\n",
      "3176/3176 [==============================] - 2s 615us/step - loss: 1.3139 - acc: 0.5290\n",
      "Epoch 49/100\n",
      "3176/3176 [==============================] - 2s 636us/step - loss: 1.2853 - acc: 0.5428\n",
      "Epoch 50/100\n",
      "3176/3176 [==============================] - 2s 619us/step - loss: 1.2902 - acc: 0.5491\n",
      "Epoch 51/100\n",
      "3176/3176 [==============================] - 2s 619us/step - loss: 1.2654 - acc: 0.5504\n",
      "Epoch 52/100\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 1.2433 - acc: 0.5642\n",
      "Epoch 53/100\n",
      "3176/3176 [==============================] - 2s 624us/step - loss: 1.2242 - acc: 0.5737\n",
      "Epoch 54/100\n",
      "3176/3176 [==============================] - 2s 626us/step - loss: 1.1983 - acc: 0.5778\n",
      "Epoch 55/100\n",
      "3176/3176 [==============================] - 2s 633us/step - loss: 1.1923 - acc: 0.5775\n",
      "Epoch 56/100\n",
      "3176/3176 [==============================] - 2s 616us/step - loss: 1.1859 - acc: 0.5841\n",
      "Epoch 57/100\n",
      "3176/3176 [==============================] - 2s 633us/step - loss: 1.1650 - acc: 0.5960\n",
      "Epoch 58/100\n",
      "3176/3176 [==============================] - 2s 615us/step - loss: 1.1358 - acc: 0.6039\n",
      "Epoch 59/100\n",
      "3176/3176 [==============================] - 2s 642us/step - loss: 1.1170 - acc: 0.6111\n",
      "Epoch 60/100\n",
      "3176/3176 [==============================] - 2s 707us/step - loss: 1.0936 - acc: 0.6228\n",
      "Epoch 61/100\n",
      "3176/3176 [==============================] - 2s 630us/step - loss: 1.0834 - acc: 0.6269\n",
      "Epoch 62/100\n",
      "3176/3176 [==============================] - 2s 620us/step - loss: 1.0512 - acc: 0.6373\n",
      "Epoch 63/100\n",
      "3176/3176 [==============================] - 2s 617us/step - loss: 1.0433 - acc: 0.6445\n",
      "Epoch 64/100\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 1.0092 - acc: 0.6644\n",
      "Epoch 65/100\n",
      "3176/3176 [==============================] - 2s 611us/step - loss: 0.9976 - acc: 0.6644\n",
      "Epoch 66/100\n",
      "3176/3176 [==============================] - 2s 636us/step - loss: 0.9820 - acc: 0.6653\n",
      "Epoch 67/100\n",
      "3176/3176 [==============================] - 2s 653us/step - loss: 0.9682 - acc: 0.6763\n",
      "Epoch 68/100\n",
      "3176/3176 [==============================] - 2s 635us/step - loss: 0.9288 - acc: 0.6940\n",
      "Epoch 69/100\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 0.9250 - acc: 0.6908\n",
      "Epoch 70/100\n",
      "3176/3176 [==============================] - 2s 614us/step - loss: 0.8854 - acc: 0.7125\n",
      "Epoch 71/100\n",
      "3176/3176 [==============================] - 2s 629us/step - loss: 0.8715 - acc: 0.7135\n",
      "Epoch 72/100\n",
      "3176/3176 [==============================] - 2s 605us/step - loss: 0.8481 - acc: 0.7220\n",
      "Epoch 73/100\n",
      "3176/3176 [==============================] - 2s 619us/step - loss: 0.8171 - acc: 0.7415\n",
      "Epoch 74/100\n",
      "3176/3176 [==============================] - 2s 622us/step - loss: 0.8061 - acc: 0.7355\n",
      "Epoch 75/100\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 0.7768 - acc: 0.7550\n",
      "Epoch 76/100\n",
      "3176/3176 [==============================] - 2s 613us/step - loss: 0.7512 - acc: 0.7642\n",
      "Epoch 77/100\n",
      "3176/3176 [==============================] - 2s 621us/step - loss: 0.7279 - acc: 0.7768\n",
      "Epoch 78/100\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 0.7155 - acc: 0.7843\n",
      "Epoch 79/100\n",
      "3176/3176 [==============================] - 2s 650us/step - loss: 0.7015 - acc: 0.7878\n",
      "Epoch 80/100\n",
      "3176/3176 [==============================] - 2s 630us/step - loss: 0.6695 - acc: 0.8035\n",
      "Epoch 81/100\n",
      "3176/3176 [==============================] - 2s 625us/step - loss: 0.6417 - acc: 0.8127\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3176/3176 [==============================] - 2s 619us/step - loss: 0.6307 - acc: 0.8117\n",
      "Epoch 83/100\n",
      "3176/3176 [==============================] - 2s 620us/step - loss: 0.6006 - acc: 0.8268\n",
      "Epoch 84/100\n",
      "3176/3176 [==============================] - 2s 635us/step - loss: 0.5810 - acc: 0.8372\n",
      "Epoch 85/100\n",
      "3176/3176 [==============================] - 2s 631us/step - loss: 0.5670 - acc: 0.8470\n",
      "Epoch 86/100\n",
      "3176/3176 [==============================] - 2s 634us/step - loss: 0.5429 - acc: 0.8514\n",
      "Epoch 87/100\n",
      "3176/3176 [==============================] - 2s 618us/step - loss: 0.5155 - acc: 0.8630\n",
      "Epoch 88/100\n",
      "3176/3176 [==============================] - 2s 623us/step - loss: 0.5048 - acc: 0.8665\n",
      "Epoch 89/100\n",
      "3176/3176 [==============================] - 2s 637us/step - loss: 0.4809 - acc: 0.8750\n",
      "Epoch 90/100\n",
      "3176/3176 [==============================] - 2s 639us/step - loss: 0.4851 - acc: 0.8759\n",
      "Epoch 91/100\n",
      "3176/3176 [==============================] - 2s 619us/step - loss: 0.4470 - acc: 0.8923\n",
      "Epoch 92/100\n",
      "3176/3176 [==============================] - 2s 631us/step - loss: 0.4306 - acc: 0.8999\n",
      "Epoch 93/100\n",
      "3176/3176 [==============================] - 2s 633us/step - loss: 0.4096 - acc: 0.9081\n",
      "Epoch 94/100\n",
      "3176/3176 [==============================] - 2s 761us/step - loss: 0.3850 - acc: 0.9162\n",
      "Epoch 95/100\n",
      "3176/3176 [==============================] - 2s 702us/step - loss: 0.3828 - acc: 0.9125\n",
      "Epoch 96/100\n",
      "3176/3176 [==============================] - 2s 692us/step - loss: 0.3625 - acc: 0.9238\n",
      "Epoch 97/100\n",
      "3176/3176 [==============================] - 2s 616us/step - loss: 0.3412 - acc: 0.9326\n",
      "Epoch 98/100\n",
      "3176/3176 [==============================] - 2s 616us/step - loss: 0.3206 - acc: 0.9408\n",
      "Epoch 99/100\n",
      "3176/3176 [==============================] - 2s 612us/step - loss: 0.2989 - acc: 0.9443\n",
      "Epoch 100/100\n",
      "3176/3176 [==============================] - 2s 654us/step - loss: 0.2919 - acc: 0.9499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cce0674f98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355,355,12356,200\n",
      "\n",
      "174,192,41600\n",
      "256,300,64443\n",
      "256,280,64756\n",
      "220,286,66709\n",
      "488,64,670,551924164\n",
      "2333,38,84770\n",
      "\n",
      "256,28,47405\n",
      "256,320,74433\n",
      "158,288,47598\n",
      "256,280,72551\n",
      "202,140,52950\n",
      "256,44,19004\n",
      "256,192,19888\n",
      "336,171,15101\n",
      "340,234,15667\n",
      "210,232,88077\n",
      "473,391,87830\n",
      "409,118,84007\n",
      "339,318,80085\n",
      "439,919,94775\n",
      "444,144,831,1\n",
      "220,2362,2593\n",
      "110,224,37551\n",
      "332,240,22555\n",
      "160,240,52299\n",
      "312,122,43268\n",
      "156,128,40906\n",
      "332,286,70859\n",
      "128,292,22788\n",
      "256,192,27840\n",
      "356,492,74541\n",
      "272,88,26666\n",
      "256,192,88473\n",
      "336,192,47811\n",
      "320,336,80864\n",
      "279,219,87670\n",
      "330,192,66403\n",
      "256,300,64453\n",
      "256,280,64556\n",
      "220,180,92775\n",
      "58,444,3732\n",
      "256,1404,3251,244,12\n",
      "2\n",
      "\n",
      "40,286,99911\n",
      "422,43,39518\n",
      "224,,76,703\n",
      "256,256,5640\n",
      "\n",
      "429,192,30155\n",
      "392,104,22741\n",
      "364,288,26588\n",
      "336,291,84107\n",
      "433,111,40157\n",
      "144,\n",
      "36,02301\n",
      "448,248,3651,\n",
      "252\n",
      "2604,112,19984\n",
      "6,14172622,3,32,400\n",
      "15248,408,2271\n",
      "\n",
      "280,288,7989\n",
      "\n",
      "48,140,87071\n",
      "39,2111,85298\n",
      "332,305,40447\n",
      "176,292,40993\n",
      "132,88,40335\n",
      "256,280,42459\n",
      "328,288,8299\n",
      "\n",
      "256,57,30464\n",
      "256,192,34751\n",
      "172,336,85887\n",
      "336,215,79166\n",
      "256,200,48429,2562\n",
      "60,2\n"
     ]
    }
   ],
   "source": [
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "        # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text\n",
    "\n",
    "# test start of rhyme\n",
    "print(generate_seq(model, mapping, length, '355,355,12356', 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
